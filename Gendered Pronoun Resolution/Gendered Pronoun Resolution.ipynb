{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import warnings\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\" \n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gensim\n",
    "import scipy\n",
    "import json\n",
    "import nltk\n",
    "import sys\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib: 2.0.2\n",
      "scipy: 1.2.1\n",
      "seaborn: 0.7.1\n",
      "pandas: 0.20.1\n",
      "numpy: 1.16.3\n",
      "Python: 3.6.8 |Anaconda custom (64-bit)| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('Python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "#warnings.filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test_stage_2.tsv', 'test_stage_1.tsv', 'sample_submission_stage_1.csv', 'sample_submission_stage_2.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gendered_pronoun_df = pd.read_csv('input/test_stage_1.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('input/sample_submission_stage_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2  development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3  development-4  The current members of Crime have also perform...     his   \n",
       "4  development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "\n",
       "   Pronoun-offset                  A  A-offset                B  B-offset  \\\n",
       "0             274     Cheryl Cassidy       191          Pauline       207   \n",
       "1             284          MacKenzie       228    Bernard Leach       251   \n",
       "2             265            Angeloz       173       De la Sota       246   \n",
       "3             321               Hell       174  Henry Rosenthal       336   \n",
       "4             437  Kitty Oppenheimer       219           Rivera       294   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://en.wikipedia.org/wiki/List_of_Teachers_...  \n",
       "1      http://en.wikipedia.org/wiki/Warren_MacKenzie  \n",
       "2  http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...  \n",
       "3          http://en.wikipedia.org/wiki/Crime_(band)  \n",
       "4        http://en.wikipedia.org/wiki/Jessica_Rivera  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.Text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 9 columns):\n",
      "ID                2000 non-null object\n",
      "Text              2000 non-null object\n",
      "Pronoun           2000 non-null object\n",
      "Pronoun-offset    2000 non-null int64\n",
      "A                 2000 non-null object\n",
      "A-offset          2000 non-null int64\n",
      "B                 2000 non-null object\n",
      "B-offset          2000 non-null int64\n",
      "URL               2000 non-null object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 140.7+ KB\n"
     ]
    }
   ],
   "source": [
    "gendered_pronoun_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = gendered_pronoun_df.drop(['URL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2  development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3  development-4  The current members of Crime have also perform...     his   \n",
       "4  development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "\n",
       "   Pronoun-offset                  A  A-offset                B  B-offset  \n",
       "0             274     Cheryl Cassidy       191          Pauline       207  \n",
       "1             284          MacKenzie       228    Bernard Leach       251  \n",
       "2             265            Angeloz       173       De la Sota       246  \n",
       "3             321               Hell       174  Henry Rosenthal       336  \n",
       "4             437  Kitty Oppenheimer       219           Rivera       294  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='blue'>**The goal is to give the probabilities for the 'Pronoun' in 'Text' to refer to 'A' or 'B' or 'Neither'.\n",
    "The train data is not labeled in this challenge.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features\n",
    "(We will end up not using these features. This is here just just to play with the data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the (pedagogical) kaggle kernel https://www.kaggle.com/mjbahmani/top-3-nlp-libraries-tutorial-nltk-spacy-gensim ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"num_words\"] = train_df[\"Text\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_words in data_df 204\n",
      "min of num_words in data_df 16\n"
     ]
    }
   ],
   "source": [
    "print('maximum of num_words in data_df',train_df[\"num_words\"].max())\n",
    "print('min of num_words in data_df',train_df[\"num_words\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_unique_words in train 136\n",
      "mean of num_unique_words in data_df 56.4005\n"
     ]
    }
   ],
   "source": [
    "train_df[\"num_unique_words\"] = train_df[\"Text\"].apply(lambda x: len(set(str(x).split())))\n",
    "print('maximum of num_unique_words in train',train_df[\"num_unique_words\"].max())\n",
    "print('mean of num_unique_words in data_df',train_df[\"num_unique_words\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_chars in data_df 1270\n"
     ]
    }
   ],
   "source": [
    "train_df[\"num_chars\"] = train_df[\"Text\"].apply(lambda x: len(str(x)))\n",
    "print('maximum of num_chars in data_df',train_df[\"num_chars\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Benjamin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_stopwords in data_df 88\n"
     ]
    }
   ],
   "source": [
    "train_df[\"num_stopwords\"] = train_df[\"Text\"].apply(lambda x: len([w for w in str(x).lower().split() \n",
    "                                                                  if w in eng_stopwords]))\n",
    "\n",
    "print('maximum of num_stopwords in data_df',train_df[\"num_stopwords\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_punctuations in data_df 93\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "train_df[\"num_punctuations\"] =train_df['Text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "print('maximum of num_punctuations in data_df',train_df[\"num_punctuations\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of title case words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_words_upper in data_df 11\n"
     ]
    }
   ],
   "source": [
    "train_df[\"num_words_upper\"] = train_df[\"Text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "print('maximum of num_words_upper in data_df',train_df[\"num_words_upper\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'B',\n",
      "       'B-offset', 'num_words', 'num_unique_words', 'num_chars',\n",
      "       'num_stopwords', 'num_punctuations', 'num_words_upper'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "      <td>426</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>410</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>95</td>\n",
       "      <td>71</td>\n",
       "      <td>536</td>\n",
       "      <td>39</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2  development-3  He had been reelected to Congress, but resigne...     his   \n",
       "\n",
       "   Pronoun-offset               A  A-offset              B  B-offset  \\\n",
       "0             274  Cheryl Cassidy       191        Pauline       207   \n",
       "1             284       MacKenzie       228  Bernard Leach       251   \n",
       "2             265         Angeloz       173     De la Sota       246   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         74                61        426             29                14   \n",
       "1         65                58        410             22                12   \n",
       "2         95                71        536             39                16   \n",
       "\n",
       "   num_words_upper  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pronoun binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pronoun = train_df.Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['He', 'Her', 'His', 'She', 'he', 'her', 'him', 'his', 'she'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>Pronoun_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "      <td>426</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>410</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>95</td>\n",
       "      <td>71</td>\n",
       "      <td>536</td>\n",
       "      <td>39</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>69</td>\n",
       "      <td>58</td>\n",
       "      <td>401</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>112</td>\n",
       "      <td>80</td>\n",
       "      <td>660</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2  development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3  development-4  The current members of Crime have also perform...     his   \n",
       "4  development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "\n",
       "   Pronoun-offset                  A  A-offset                B  B-offset  \\\n",
       "0             274     Cheryl Cassidy       191          Pauline       207   \n",
       "1             284          MacKenzie       228    Bernard Leach       251   \n",
       "2             265            Angeloz       173       De la Sota       246   \n",
       "3             321               Hell       174  Henry Rosenthal       336   \n",
       "4             437  Kitty Oppenheimer       219           Rivera       294   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         74                61        426             29                14   \n",
       "1         65                58        410             22                12   \n",
       "2         95                71        536             39                16   \n",
       "3         69                58        401             24                13   \n",
       "4        112                80        660             43                18   \n",
       "\n",
       "   num_words_upper  Pronoun_binary  \n",
       "0                0             4.0  \n",
       "1                0             2.0  \n",
       "2                0             2.0  \n",
       "3                1             2.0  \n",
       "4                1             1.0  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary = {\n",
    "    \"He\": 0,\n",
    "    \"he\": 0,\n",
    "    \"She\": 1,\n",
    "    \"she\": 1,\n",
    "    \"His\": 2,\n",
    "    \"his\": 2,\n",
    "    \"Him\": 3,\n",
    "    \"him\": 3,\n",
    "    \"Her\": 4,\n",
    "    \"her\": 4\n",
    "}\n",
    "for index in range(len(train_df)):\n",
    "    key = train_df.iloc[index]['Pronoun']\n",
    "    train_df.at[index, 'Pronoun_binary'] = binary[key]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matei Ionita's kernel  (using BERT embeddings)\n",
    "https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline\n",
    "\n",
    "Here we use the GAP files, which contain the training labels of the dataset (and the datasets).  \n",
    "Then we use BERT's (contextual) embeddings for the three words A, B and P, and we train a multi-layer perceptron model (with Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uncased_L-12_H-768_A-12.zip'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  uncased_L-12_H-768_A-12.zip\n",
      "   creating: uncased_L-12_H-768_A-12/\n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenization.py'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv')\n",
    "wget.download('https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv')\n",
    "wget.download('https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv')\n",
    "wget.download('https://raw.githubusercontent.com/google-research/bert/master/modeling.py')\n",
    "wget.download('https://raw.githubusercontent.com/google-research/bert/master/extract_features.py')\n",
    "wget.download('https://raw.githubusercontent.com/google-research/bert/master/tokenization.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import modeling\n",
    "import extract_features\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_offset_no_spaces(text, offset):\n",
    "    count = 0\n",
    "    for pos in range(offset):\n",
    "        if text[pos] != \" \": \n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def count_chars_no_special(text):\n",
    "    count = 0\n",
    "    special_char_list = [\"#\"]\n",
    "    for pos in range(len(text)):\n",
    "        if text[pos] not in special_char_list: \n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def count_length_no_special(text):\n",
    "    count = 0\n",
    "    special_char_list = [\"#\", \" \"]\n",
    "    for pos in range(len(text)):\n",
    "        if text[pos] not in special_char_list: \n",
    "            count +=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_bert(data):\n",
    "    '''\n",
    "    Runs a forward propagation of BERT on input text, extracting contextual word embeddings\n",
    "    Input: data, a pandas DataFrame containing the information in one of the GAP files\n",
    "\n",
    "    Output: emb, a pandas DataFrame containing contextual embeddings for the words A, B and Pronoun. Each embedding is a numpy array of shape (768)\n",
    "    columns: \"emb_A\": the embedding for word A\n",
    "             \"emb_B\": the embedding for word B\n",
    "             \"emb_P\": the embedding for the pronoun\n",
    "             \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "    '''\n",
    "    # From the current file, take the text only, and write it in a file which will be passed to BERT\n",
    "    text = data[\"Text\"]\n",
    "    text.to_csv(\"input.txt\", index = False, header = False)\n",
    "\n",
    "    # The script extract_features.py runs forward propagation through BERT, and writes the output in the file output.jsonl\n",
    "    # I'm lazy, so I'm only saving the output of the last layer. Feel free to change --layers = -1 to save the output of other layers.\n",
    "    os.system(\"python3 extract_features.py \\\n",
    "      --input_file=input.txt \\\n",
    "      --output_file=output.jsonl \\\n",
    "      --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "      --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "      --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "      --layers=-1 \\\n",
    "      --max_seq_length=256 \\\n",
    "      --batch_size=8\")\n",
    "\n",
    "    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
    "\n",
    "    os.system(\"rm output.jsonl\")\n",
    "    os.system(\"rm input.txt\")\n",
    "\n",
    "    index = data.index\n",
    "    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
    "    emb = pd.DataFrame(index = index, columns = columns)\n",
    "    emb.index.name = \"ID\"\n",
    "\n",
    "    for i in range(len(data)): # For each line in the data file\n",
    "        # get the words A, B, Pronoun. Convert them to lower case, since we're using the uncased version of BERT\n",
    "        P = data.loc[i,\"Pronoun\"].lower()\n",
    "        A = data.loc[i,\"A\"].lower()\n",
    "        B = data.loc[i,\"B\"].lower()\n",
    "\n",
    "        # For each word, find the offset not counting spaces. This is necessary for comparison with the output of BERT\n",
    "        P_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n",
    "        A_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n",
    "        B_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n",
    "        # Figure out the length of A, B, not counting spaces or special characters\n",
    "        A_length = count_length_no_special(A)\n",
    "        B_length = count_length_no_special(B)\n",
    "\n",
    "        # Initialize embeddings with zeros\n",
    "        emb_A = np.zeros(768)\n",
    "        emb_B = np.zeros(768)\n",
    "        emb_P = np.zeros(768)\n",
    "\n",
    "        # Initialize counts\n",
    "        count_chars = 0\n",
    "        cnt_A, cnt_B, cnt_P = 0, 0, 0\n",
    "\n",
    "        features = pd.DataFrame(bert_output.loc[i,\"features\"]) # Get the BERT embeddings for the current line in the data file\n",
    "        for j in range(2,len(features)):  # Iterate over the BERT tokens for the current line; we skip over the first 2 tokens, which don't correspond to words\n",
    "            token = features.loc[j,\"token\"]\n",
    "\n",
    "            # See if the character count until the current token matches the offset of any of the 3 target words\n",
    "            if count_chars  == P_offset: \n",
    "                # print(token)\n",
    "                emb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_P += 1\n",
    "            if count_chars in range(A_offset, A_offset + A_length): \n",
    "                # print(token)\n",
    "                emb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_A +=1\n",
    "            if count_chars in range(B_offset, B_offset + B_length): \n",
    "                # print(token)\n",
    "                emb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_B +=1\t\t\t\t\t\t\t\t\n",
    "            # Update the character count\n",
    "            count_chars += count_length_no_special(token)\n",
    "        # Taking the average between tokens in the span of A or B, so divide the current value by the count\t\n",
    "        if cnt_A > 0:\n",
    "            emb_A /= cnt_A   \n",
    "        if cnt_B > 0:\n",
    "            emb_B /= cnt_B\n",
    "\n",
    "        # Work out the label of the current piece of text\n",
    "        label = \"Neither\"\n",
    "        if (data.loc[i,\"A-coref\"] == True):\n",
    "            label = \"A\"\n",
    "        if (data.loc[i,\"B-coref\"] == True):\n",
    "            label = \"B\"\n",
    "\n",
    "        # Put everything together in emb\n",
    "        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at  Sat Aug  3 11:57:34 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Benjamin/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/Benjamin/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at  Sat Aug  3 13:29:22 2019\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "print(\"Started at \", time.ctime())\n",
    "test_data = pd.read_csv(\"gap-test.tsv\", sep = '\\t')\n",
    "test_emb = run_bert(test_data)\n",
    "test_emb.to_json(\"contextual_embeddings_gap_test.json\", orient = 'columns')\n",
    "\n",
    "validation_data = pd.read_csv(\"gap-validation.tsv\", sep = '\\t')\n",
    "validation_emb = run_bert(validation_data)\n",
    "validation_emb.to_json(\"contextual_embeddings_gap_validation.json\", orient = 'columns')\n",
    "\n",
    "development_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\n",
    "development_emb = run_bert(development_data)\n",
    "development_emb.to_json(\"contextual_embeddings_gap_development.json\", orient = 'columns')\n",
    "print(\"Finished at \", time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>True</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>True</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>False</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>False</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>False</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                               Text Pronoun  \\\n",
       "ID                                                                             \n",
       "0   development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1   development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2   development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3   development-4  The current members of Crime have also perform...     his   \n",
       "4   development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "\n",
       "    Pronoun-offset                  A  A-offset  A-coref                B  \\\n",
       "ID                                                                          \n",
       "0              274     Cheryl Cassidy       191     True          Pauline   \n",
       "1              284          MacKenzie       228     True    Bernard Leach   \n",
       "2              265            Angeloz       173    False       De la Sota   \n",
       "3              321               Hell       174    False  Henry Rosenthal   \n",
       "4              437  Kitty Oppenheimer       219    False           Rivera   \n",
       "\n",
       "    B-offset  B-coref                                                URL  \n",
       "ID                                                                        \n",
       "0        207    False  http://en.wikipedia.org/wiki/List_of_Teachers_...  \n",
       "1        251    False      http://en.wikipedia.org/wiki/Warren_MacKenzie  \n",
       "2        246     True  http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...  \n",
       "3        336     True          http://en.wikipedia.org/wiki/Crime_(band)  \n",
       "4        294     True        http://en.wikipedia.org/wiki/Jessica_Rivera  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "development_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_A</th>\n",
       "      <th>emb_B</th>\n",
       "      <th>emb_P</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.7388775000000001, -0.22716449999999996, 0.1...</td>\n",
       "      <td>[0.385269, 0.024027999999999997, 0.90755000000...</td>\n",
       "      <td>[-0.008841, -0.9525579999999999, 0.113496, -0....</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.834177, -0.47442199999999995, 1.149044, 0.1...</td>\n",
       "      <td>[-0.0622775, 0.21735149999999998, -0.366814, -...</td>\n",
       "      <td>[-0.316778, 0.9291269999999999, -0.61344599999...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.016817499999999985, -0.47463500000000003, 0...</td>\n",
       "      <td>[-0.2066525, -0.58868, 0.57271025, 0.266884, 0...</td>\n",
       "      <td>[-0.6279279999999999, -0.401219, -0.140529, 0....</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.35994299999999996, 0.569445, -0.094911, 0.1...</td>\n",
       "      <td>[-0.1316495, -0.06583699999999999, 0.313922, -...</td>\n",
       "      <td>[0.19959000000000002, 0.226825, -0.291493, 0.2...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.192178, -0.22681325000000002, 0.1195679999...</td>\n",
       "      <td>[0.293198, -0.609892, -0.09239000000000001, -0...</td>\n",
       "      <td>[-0.011042999999999999, -1.187912, -0.898381, ...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                emb_A  \\\n",
       "ID                                                      \n",
       "0   [0.7388775000000001, -0.22716449999999996, 0.1...   \n",
       "1   [0.834177, -0.47442199999999995, 1.149044, 0.1...   \n",
       "2   [0.016817499999999985, -0.47463500000000003, 0...   \n",
       "3   [0.35994299999999996, 0.569445, -0.094911, 0.1...   \n",
       "4   [-0.192178, -0.22681325000000002, 0.1195679999...   \n",
       "\n",
       "                                                emb_B  \\\n",
       "ID                                                      \n",
       "0   [0.385269, 0.024027999999999997, 0.90755000000...   \n",
       "1   [-0.0622775, 0.21735149999999998, -0.366814, -...   \n",
       "2   [-0.2066525, -0.58868, 0.57271025, 0.266884, 0...   \n",
       "3   [-0.1316495, -0.06583699999999999, 0.313922, -...   \n",
       "4   [0.293198, -0.609892, -0.09239000000000001, -0...   \n",
       "\n",
       "                                                emb_P label  \n",
       "ID                                                           \n",
       "0   [-0.008841, -0.9525579999999999, 0.113496, -0....     A  \n",
       "1   [-0.316778, 0.9291269999999999, -0.61344599999...     A  \n",
       "2   [-0.6279279999999999, -0.401219, -0.140529, 0....     B  \n",
       "3   [0.19959000000000002, 0.226825, -0.291493, 0.2...     B  \n",
       "4   [-0.011042999999999999, -1.187912, -0.898381, ...     B  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(development_emb)\n",
    "development_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 768 768\n"
     ]
    }
   ],
   "source": [
    "print(development_emb.loc[0,'emb_A'].size, validation_emb.loc[0,'emb_A'].size, test_emb.loc[0,'emb_A'].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model (mutli-layer perceptron) and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\n",
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "\n",
    "# Parameters of the model\n",
    "\n",
    "dense_layer_sizes = [64,32]\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "n_fold = 5\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "patience = 100\n",
    "# n_test = 100\n",
    "lambd = 0.1 # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    for i in range(1,len(dense_layer_sizes)):\n",
    "        X = Dense(dense_layer_sizes[i])(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = Dropout(dropout_rate, seed=11)(X)\n",
    "        \n",
    "    X = Dense(3, name='output', kernel_regularizer=regularizers.l2(lambd))(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = models.Model(inputs = X_input, outputs = X, name = \"clf_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_json(embeddings):\n",
    "\t'''\n",
    "\tParses the embeddigns given by BERT, and suitably formats them to be passed to the MLP model\n",
    "\n",
    "\tInput: embeddings, a DataFrame containing contextual embeddings from BERT, as well as the labels for the classification problem\n",
    "\tcolumns: \"emb_A\": contextual embedding for the word A\n",
    "\t         \"emb_B\": contextual embedding for the word B\n",
    "\t         \"emb_P\": contextual embedding for the pronoun\n",
    "\t         \"label\": the answer to the coreference problem: \"A\", \"B\" or \"NEITHER\"\n",
    "\n",
    "\tOutput: X, a numpy array containing, for each line in the GAP file, the concatenation of the embeddings of the target words\n",
    "\t        Y, a numpy array containing, for each line in the GAP file, the one-hot encoded answer to the coreference problem\n",
    "\t'''\n",
    "\tembeddings.sort_index(inplace = True) # Sorting the DataFrame, because reading from the json file messed with the order\n",
    "\tX = np.zeros((len(embeddings),3*768))\n",
    "\tY = np.zeros((len(embeddings), 3))\n",
    "\n",
    "\t# Concatenate features\n",
    "\tfor i in range(len(embeddings)):\n",
    "\t\tA = np.array(embeddings.loc[i,\"emb_A\"])\n",
    "\t\tB = np.array(embeddings.loc[i,\"emb_B\"])\n",
    "\t\tP = np.array(embeddings.loc[i,\"emb_P\"])\n",
    "\t\tX[i] = np.concatenate((A,B,P))\n",
    "\n",
    "\t# One-hot encoding for labels\n",
    "\tfor i in range(len(embeddings)):\n",
    "\t\tlabel = embeddings.loc[i,\"label\"]\n",
    "\t\tif label == \"A\":\n",
    "\t\t\tY[i,0] = 1\n",
    "\t\telif label == \"B\":\n",
    "\t\t\tY[i,1] = 1\n",
    "\t\telse:\n",
    "\t\t\tY[i,2] = 1\n",
    "\n",
    "\treturn X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read development embeddigns from json file - this is the output of Bert\n",
    "development = pd.read_json(\"contextual_embeddings_gap_development.json\")\n",
    "X_development, Y_development = parse_json(development)   # This will be used for testing\n",
    "\n",
    "validation = pd.read_json(\"contextual_embeddings_gap_validation.json\")\n",
    "X_validation, Y_validation = parse_json(validation)   ## This will be used for training\n",
    "\n",
    "test = pd.read_json(\"contextual_embeddings_gap_test.json\")\n",
    "X_test, Y_test = parse_json(test)   ## This will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There may be a few NaN values, where the offset of a target word is greater than the max_seq_length of BERT.\n",
    "# They are very few, so I'm just dropping the rows.\n",
    "remove_test = [row for row in range(len(X_test)) if np.sum(np.isnan(X_test[row]))]\n",
    "X_test = np.delete(X_test, remove_test, 0)\n",
    "Y_test = np.delete(Y_test, remove_test, 0)\n",
    "\n",
    "remove_validation = [row for row in range(len(X_validation)) if np.sum(np.isnan(X_validation[row]))]\n",
    "X_validation = np.delete(X_validation, remove_validation, 0)\n",
    "Y_validation = np.delete(Y_validation, remove_validation, 0)\n",
    "\n",
    "# We want predictions for all development rows. So instead of removing rows, make them 0\n",
    "remove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row]))]\n",
    "X_development[remove_development] = np.zeros(3*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Will train on data from the gap-test and gap-validation files, in total 2454 rows\n",
    "X_train = np.concatenate((X_test, X_validation), axis = 0)\n",
    "Y_train = np.concatenate((Y_test, Y_validation), axis = 0)\n",
    "\n",
    "# Will predict probabilities for data from the gap-development file; initializing the predictions\n",
    "prediction = np.zeros((len(X_development),3)) # testing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Aug  7 14:27:38 2019\n",
      "Fold 1 started at Wed Aug  7 14:28:08 2019\n",
      "Fold 2 started at Wed Aug  7 14:28:38 2019\n",
      "Fold 3 started at Wed Aug  7 14:29:09 2019\n",
      "Fold 4 started at Wed Aug  7 14:29:38 2019\n",
      "CV mean score: 0.5767, std: 0.0276.\n",
      "[0.5876452079185762, 0.5256018349161095, 0.5831299087374334, 0.5785930949987901, 0.6087202651756436]\n",
      "Test score: 0.5320775842295536\n"
     ]
    }
   ],
   "source": [
    "# Training and cross-validation\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "scores = []\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n",
    "\t# split training and validation data\n",
    "\tprint('Fold', fold_n, 'started at', time.ctime())\n",
    "\tX_tr, X_val = X_train[train_index], X_train[valid_index]\n",
    "\tY_tr, Y_val = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "\t# Define the model, re-initializing for each fold\n",
    "\tclassif_model = build_mlp_model([X_train.shape[1]])\n",
    "\tclassif_model.compile(optimizer = optimizers.Adam(lr = learning_rate), loss = \"categorical_crossentropy\")\n",
    "\tcallbacks = [kc.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights = True)]\n",
    "\n",
    "\t# train the model\n",
    "\tclassif_model.fit(x = X_tr, y = Y_tr, epochs = epochs, batch_size = batch_size, \n",
    "                      callbacks = callbacks, validation_data = (X_val, Y_val), verbose = 0)\n",
    "\n",
    "\t# make predictions on validation and test data\n",
    "\tpred_valid = classif_model.predict(x = X_val, verbose = 0)\n",
    "\tpred = classif_model.predict(x = X_development, verbose = 0)\n",
    "\n",
    "\t# oof[valid_index] = pred_valid.reshape(-1,)\n",
    "\tscores.append(log_loss(Y_val, pred_valid))\n",
    "\tprediction += pred\n",
    "prediction /= n_fold\n",
    "\n",
    "# Print CV scores, as well as score on the test data\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "print(scores)\n",
    "print(\"Test score:\", log_loss(Y_development,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the prediction to file for submission at stage 1\n",
    "submission = pd.read_csv(\"input/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
    "submission[\"A\"] = prediction[:,0]\n",
    "submission[\"B\"] = prediction[:,1]\n",
    "submission[\"NEITHER\"] = prediction[:,2]\n",
    "submission.to_csv(\"submission_bert_stage_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stage 2 test data\n",
    "stage_2_data = pd.read_csv(\"input/test_stage_2.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_bert_no_labels(data):\n",
    "\n",
    "    text = data[\"Text\"]\n",
    "    text.to_csv(\"input.txt\", index = False, header = False)\n",
    "\n",
    "    os.system(\"python3 extract_features.py \\\n",
    "      --input_file=input.txt \\\n",
    "      --output_file=output.jsonl \\\n",
    "      --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "      --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "      --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "      --layers=-1 \\\n",
    "      --max_seq_length=256 \\\n",
    "      --batch_size=8\")\n",
    "\n",
    "    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
    "\n",
    "    os.system(\"rm output.jsonl\")\n",
    "    os.system(\"rm input.txt\")\n",
    "\n",
    "    index = data.index\n",
    "    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
    "    emb = pd.DataFrame(index = index, columns = columns)\n",
    "    emb.index.name = \"ID\"\n",
    "\n",
    "    for i in range(len(data)): \n",
    "        P = data.loc[i,\"Pronoun\"].lower()\n",
    "        A = data.loc[i,\"A\"].lower()\n",
    "        B = data.loc[i,\"B\"].lower()\n",
    "\n",
    "        P_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"Pronoun-offset\"])\n",
    "        A_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"A-offset\"])\n",
    "        B_offset = compute_offset_no_spaces(data.loc[i,\"Text\"], data.loc[i,\"B-offset\"])\n",
    "\n",
    "        A_length = count_length_no_special(A)\n",
    "        B_length = count_length_no_special(B)\n",
    "\n",
    "        emb_A = np.zeros(768)\n",
    "        emb_B = np.zeros(768)\n",
    "        emb_P = np.zeros(768)\n",
    "\n",
    "        # Initialize counts\n",
    "        count_chars = 0\n",
    "        cnt_A, cnt_B, cnt_P = 0, 0, 0\n",
    "\n",
    "        features = pd.DataFrame(bert_output.loc[i,\"features\"]) \n",
    "        for j in range(2,len(features)): \n",
    "            token = features.loc[j,\"token\"]\n",
    "            \n",
    "            if count_chars  == P_offset: \n",
    "                emb_P += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_P += 1\n",
    "            if count_chars in range(A_offset, A_offset + A_length): \n",
    "                emb_A += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_A +=1\n",
    "            if count_chars in range(B_offset, B_offset + B_length): \n",
    "                emb_B += np.array(features.loc[j,\"layers\"][0]['values'])\n",
    "                cnt_B +=1\t\t\t\t\t\t\t\t\n",
    "            count_chars += count_length_no_special(token)\n",
    "        if cnt_A > 0:\n",
    "            emb_A /= cnt_A   \n",
    "        if cnt_B > 0:\n",
    "            emb_B /= cnt_B\n",
    "\n",
    "        emb.iloc[i] = [emb_A, emb_B, emb_P]\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes too long!\n",
    "print(\"Started at \", time.ctime())\n",
    "stage_2_emb = run_bert_no_labels(stage_2_data)\n",
    "print(\"Finished at \", time.ctime())\n",
    "stage_2_emb.to_json(\"contextual_embeddings_stage_2_test.json\", orient = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Unfortunately, the stage 2 data is to big to be processed on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
